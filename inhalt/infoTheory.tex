\section{Information Theory}
\subsection*{Information Content (Surprisal)}
$-log_2 p(x)$ {\tiny the more frequent the word, the lower its information content.
e.g. the word type “blue” occurs ca. 3750 times in 10000 tokens, and its information content is $-log_2(3750/10000) \approx$1.42 bits.}
\subsection*{Shannon Entropy}
$H(X) = - \sum p(x) log_2 p(x)$ {\tiny entropy as probability, the average information content of information encoding units in the language. Measure of information encoding potential of a symbol system.}\\
{\tiny The higher the uncertainty, the larger the entropy. e.g. $H_{char}(Morse) = - (\frac{86}{136} * log_2(\frac{86}{136}) + \frac{50}{136} * log_2(\frac{50}{136})) \approx 0.949$ bits per character.} \\
{\tiny A series of studies proposed to use entropic measures to distinguish human writing from other types of symbol systems.}
\subsection*{Joint Entropy, Conditional Entropy}
{\scriptsize Joint Entropy:} $H(X, Y) = - \sum \sum p(x, y) log_2 p(x, y)$ \\
{\scriptsize Conditional Entropy:} $H(Y|X) = - \sum p(x) \sum p(y|x) log_2 p(y|x)$ {\tiny The more ambiguity in language (uncertainty), the higher conditional entropy. No ambiguity -> 0.}\\
{\tiny Ackerman \& Malouf (2013) propose two entropic measures for morphological complexity: the average entropy as e-complexity, and the average conditional entropy as i-complexity measure. They are related to learnability.}
\subsection*{Probability Estimation}
{\scriptsize Maximum Likelihood (ML) \\
Problems: unit problem, sample size problem, interdependence problem, extrapolation problem \\
Methods: frequency-based, language models, experiments with humans}
\subsection*{Mutual Information}
$I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$ {\tiny reduction in the uncertainty of X given Y. \\
compromise between minimum learning cost H(Y) and maximum expliciteness I(X; Y). \\
Entropy is the upper bound on the mutual information between forms and meanings \\
Is the entropy rate zero? -> asymptotic determinism of human utterances.}