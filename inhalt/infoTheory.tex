\section{Information Theory}

\subsection*{Information Content (Surprisal)}
$-log_2 p(x)$: the more frequent the word, the lower its information content.
e.g. the word type “blue” occurs ca. 3750 times in
10000 tokens, and its information content is $-log_2(3750/10000) \approx$1.42 bits.

\subsection*{Shannon Entropy}
$H(X) = - \sum p(x) log_2 p(x)$: entropy as probability, the average information content of information encoding units in the language. Measure of information encoding potential of a symbol system. The higher the uncertainty, the larger the entropy. e.g. $H_{char}(Morse) = - (\frac{86}{136} * log_2(\frac{86}{136}) + \frac{50}{136} * log_2(\frac{50}{136})) \approx 0.949$ bits per character. \\

\subsection*{Joint Entropy, Conditional Entropy}
Joint Entropy: $H(X, Y) = - \sum \sum p(x, y) log_2 p(x, y)$ \\
Conditional Entropy: $H(Y|X) = - \sum p(x) \sum p(y|x) log_2 p(y|x)$ The more ambiguity in language (uncertainty), the higher conditional entropy. No ambiguity -> 0.

\subsection*{Probability Estimation}
Maximum Likelihood (ML) \\
Problems: unit problem, sample size problem, interdependence problem, extrapolation problem \\
Methods: frequency-based, language models, experiments with humans

\subsection*{Mutual Information}
$I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$ reduction in the uncertainty of X given Y. \\
compromise between minimum learning cost H(Y) and maximum expliciteness I(X; Y). \\
Entropy is the upper bound on the mutual information between forms and meanings \\
Is the entropy rate zero? -> asymptotic determinism of human utterances.







